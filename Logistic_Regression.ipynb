{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic Regression.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "ZIFVv9mEteSU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train_prime, y_train, X_test_prime, y_test, classes = load_dataset()\n",
        "\n",
        "X_train_flatten = X_train_prime.reshape(64*64*3, 209)\n",
        "X_test_flatten = X_test_prime.reshape(64*64*3, 50)\n",
        "\n",
        "X_train = X_train_flatten/255\n",
        "X_test = X_test_flatten/255\n",
        "\n",
        "X_train.shape, X_test.shape\n",
        "\n",
        "def sigmoid(x):\n",
        "  \n",
        "  return 1/(1 + np.exp(-x))\n",
        "\n",
        "\n",
        "def tanh(x):\n",
        "  \n",
        "  return (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
        "\n",
        "\n",
        "def forward(W, X, b):\n",
        "  \n",
        "  forward = {}\n",
        "  \n",
        "  Z = np.dot(W.T, X) + b\n",
        "  A = sigmoid(Z)\n",
        "  \n",
        "  forward['Z'] = Z\n",
        "  forward['A'] = A\n",
        "  forward['W'] = W\n",
        "  forward['b'] = b\n",
        "  \n",
        "  return forward\n",
        "\n",
        "\n",
        "def backward(A, X, Y):\n",
        "  \n",
        "  backward = {} \n",
        "  m = X.shape[1]\n",
        "  \n",
        "  dZ = A - Y\n",
        "  dW = np.dot(X, dZ.T) / m\n",
        "  db = np.sum(dZ) / m\n",
        "  \n",
        "  backward['dZ'] = dZ\n",
        "  backward['dW'] = dW\n",
        "  backward['db'] = db\n",
        "  \n",
        "  return backward\n",
        "  \n",
        "def update(W, b, dW, db, learning_rate):\n",
        "  \n",
        "  W = W - learning_rate * dW\n",
        "  b = b - learning_rate * db\n",
        "  \n",
        "  return W, b\n",
        "\n",
        "def weights(X_train, y_train, learning_rate):\n",
        "  \n",
        "  for i in range(2000) :\n",
        "    \n",
        "    if(i == 0):\n",
        "      \n",
        "      W = np.zeros((12288, 1))\n",
        "      b = 0\n",
        "    \n",
        "      forward_dict = forward(W, X_train, b)\n",
        "      A, b, W = forward_dict['A'], forward_dict['b'], forward_dict['W']\n",
        "\n",
        "      backward_dict = backward(A, X_train, y_train)\n",
        "      dW, db = backward_dict['dW'], backward_dict['db']\n",
        "\n",
        "      update_dict = update(W, b, dW, db, learning_rate)\n",
        "      forward_dict['W'], forward_dict['b'] = W, b\n",
        "      print(str(i)+'th epoch Completed')  \n",
        "    \n",
        "    else :\n",
        "  \n",
        "      W, b = forward_dict['W'], forward_dict['b']\n",
        "\n",
        "      forward_dict = forward(W, X_train, b)\n",
        "      A, b, W = forward_dict['A'], forward_dict['b'], forward_dict['W']\n",
        "\n",
        "      backward_dict = backward(A, X_train, y_train)\n",
        "      dW, db = backward_dict['dW'], backward_dict['db']\n",
        "\n",
        "      W, b = update(W, b, dW, db, learning_rate)\n",
        "      forward_dict['W'], forward_dict['b'] = W, b\n",
        "      if(i%250 == 0):\n",
        "            print(str(i)+'th epoch Completed')\n",
        "    \n",
        "  return forward_dict    \n",
        "\n",
        "\n",
        "forward = weights(X_train, y_train, 0.001)\n",
        "W, b = forward['W'], forward['b']\n",
        "y_predict = np.dot(W.T, X_test) + b\n",
        "\n",
        "for i in range(50):\n",
        "        if y_predict[0,i] > .5:\n",
        "            y_predict[0, i] = 1\n",
        "        else:\n",
        "            y_predict[0, i] = 0\n",
        "\n",
        "y_predict, y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J7r_8B3pteVb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4FR-RnjTteY4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xGfNrmF3tedT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aprki5z2teiL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ALJ24_oAtemb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8fO0Ht_Dteg6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZMe8eDoCtecA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}